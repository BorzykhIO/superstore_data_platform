from pyspark.sql import SparkSession
from pyspark.sql.functions import col, from_json
from pyspark.sql.types import StructType, StringType, IntegerType, FloatType, TimestampType
import os

# üõ† –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
KAFKA_BROKER = os.getenv("KAFKA_BROKER", "kafka:29092")
KAFKA_TOPIC = os.getenv("KAFKA_TOPIC", "test_topic")  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–≤–æ–π —Ç–æ–ø–∏–∫
POSTGRES_URL = os.getenv("POSTGRES_URL", "jdbc:postgresql://postgres_dwh:5432/dwh_db")
POSTGRES_TABLE = os.getenv("POSTGRES_TABLE", "stg.superstore_data_raw")
POSTGRES_USER = os.getenv("POSTGRES_USER", "dwh_user")
POSTGRES_PASSWORD = os.getenv("POSTGRES_PASSWORD", "dwh_pass")

# üî• –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Spark
spark = SparkSession.builder \
    .appName("KafkaSparkStreaming") \
    .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
    .config("spark.sql.streaming.forceDeleteTempCheckpointLocation", "true") \
    .getOrCreate()

# üìå –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ö–µ–º—É —Å–æ–æ–±—â–µ–Ω–∏–π Kafka (–ø–æ–¥ —Ç–≤–æ–π producer)
schema = StructType() \
    .add("Row ID", IntegerType()) \
    .add("Order ID", StringType()) \
    .add("Order Date", StringType()) \
    .add("Ship Date", StringType()) \
    .add("Ship Mode", StringType()) \
    .add("Customer ID", StringType()) \
    .add("Customer Name", StringType()) \
    .add("Segment", StringType()) \
    .add("Country", StringType()) \
    .add("City", StringType()) \
    .add("State", StringType()) \
    .add("Postal Code", StringType()) \
    .add("Region", StringType()) \
    .add("Product ID", StringType()) \
    .add("Category", StringType()) \
    .add("Sub-Category", StringType()) \
    .add("Product Name", StringType()) \
    .add("Sales", FloatType()) \
    .add("Quantity", IntegerType()) \
    .add("Discount", FloatType()) \
    .add("Profit", FloatType())

# üì° –ß–∏—Ç–∞–µ–º –ø–æ—Ç–æ–∫ –∏–∑ Kafka, –≤–∫–ª—é—á–∞—è Kafka timestamp
df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", KAFKA_BROKER) \
    .option("subscribe", KAFKA_TOPIC) \
    .option("startingOffsets", "earliest") \
    .load() \
    .selectExpr("CAST(value AS STRING)", "timestamp AS kafka_timestamp") \
    .select(from_json(col("value"), schema).alias("data"), col("kafka_timestamp")) \
    .select("data.*", "kafka_timestamp")

# üõ¢ –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞–∫–µ—Ç–Ω–æ–π –∑–∞–ø–∏—Å–∏ –≤ PostgreSQL
def write_to_postgres(batch_df, batch_id):
    batch_df.write \
        .format("jdbc") \
        .option("url", POSTGRES_URL) \
        .option("dbtable", POSTGRES_TABLE) \
        .option("user", POSTGRES_USER) \
        .option("password", POSTGRES_PASSWORD) \
        .option("driver", "org.postgresql.Driver") \
        .mode("append") \
        .save()

# üîÑ –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ—Ç–æ–∫ —Å foreachBatch
query = df.writeStream \
    .outputMode("append") \
    .foreachBatch(write_to_postgres) \
    .start()

query.awaitTermination()
